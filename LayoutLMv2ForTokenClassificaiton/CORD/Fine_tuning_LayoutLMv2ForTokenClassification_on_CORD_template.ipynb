{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a474ac",
   "metadata": {},
   "source": [
    "<h1><center>Fine tuning LayoutLMv2ForTokenClassification on CORD dataset</center></h1><h1><center>Template</center></h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f9b28",
   "metadata": {},
   "source": [
    "In this notebook, we are going to fine-tune LayoutLMv2ForTokenClassification on the CORD dataset. The goal for the model is to label words appearing in recipes appropriately. This task is treated as a Named Entity Relation Extraction Task. Compared to BERT, LayoutLMv2 also incorporates visual and layout information about the tokens when encoding them into vectors. This makes the LayoutLMv2 model very powerful for document understanding tasks.\n",
    "\n",
    "LayoutLMv2 is itself an upgrade of LayoutLM. The main novelty of LayoutLMv2 is that it also pre-trains visual embeddings, whereas the original LayoutLM only adds visual embeddings during fine-tuning.\n",
    "\n",
    "Paper: https://arxiv.org/abs/2012.14740  \n",
    "Original repo: https://github.com/microsoft/unilm/tree/master/layoutlmv2\n",
    "\n",
    "Original dataset: https://github.com/clovaai/cord\n",
    "\n",
    "inspired by: Niels Rogge @ https://github.com/NielsRogge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161ec0e",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65366efb",
   "metadata": {},
   "source": [
    "First, we install the required libraries specified in the ***requirements.txt*** file. Check it out ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6542ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datasets import (\n",
    "    Array2D,\n",
    "    Array3D,\n",
    "    ClassLabel,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    "    load_metric,\n",
    ")\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    LayoutLMv2ForTokenClassification,\n",
    "    LayoutLMv2Processor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import cord\n",
    "from perspective_transformer import four_point_transform\n",
    "from utils import normalize_bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603f00f",
   "metadata": {},
   "source": [
    "## Prepare Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f31700",
   "metadata": {},
   "source": [
    "Let's load the FUNSD dataset from the HuggingFace hub. I have uploaded it to you already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1444077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datset using the imported method load_dataset. The dataset name is 'MarkusDressel/cord'\n",
    "# your code starts here\n",
    "dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aefd4176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'bboxes', 'roi', 'ner_tags', 'image_path'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'bboxes', 'roi', 'ner_tags', 'image_path'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'bboxes', 'roi', 'ner_tags', 'image_path'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d3b5e",
   "metadata": {},
   "source": [
    "As we can see, it contains a training and test split. Each example consists of an id, tokens, bounding boxes, NER tags and a path to the document image. Note: tokens might be a bit misleading here, because these are still words. We need to convert them to actual tokens (word pieces) using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3dfde8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (538727764.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1379/538727764.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    train_dataset =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test\n",
    "train_dataset = \n",
    "validation_dataset = \n",
    "test_dataset = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0623f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169428d",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18107e66",
   "metadata": {},
   "source": [
    "First, let's store the labels in a list, and create dictionaries that let us map from labels to integer indices and vice versa. The latter will be useful when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a7239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-menu.cnt', 'I-menu.discountprice', 'I-menu.etc', 'I-menu.itemsubtotal', 'I-menu.nm', 'I-menu.num', 'I-menu.price', 'I-menu.sub_cnt', 'I-menu.sub_etc', 'I-menu.sub_nm', 'I-menu.sub_price', 'I-menu.sub_unitprice', 'I-menu.unitprice', 'I-menu.vatyn', 'I-sub_total.discount_price', 'I-sub_total.etc', 'I-sub_total.othersvc_price', 'I-sub_total.service_price', 'I-sub_total.subtotal_price', 'I-sub_total.tax_price', 'I-total.cashprice', 'I-total.changeprice', 'I-total.creditcardprice', 'I-total.emoneyprice', 'I-total.menuqty_cnt', 'I-total.menutype_cnt', 'I-total.total_etc', 'I-total.total_price', 'I-void_menu.nm', 'I-void_menu.price']\n"
     ]
    }
   ],
   "source": [
    "# get the labels from the dataset. You can find them in the feature attribute of the train_dataset\n",
    "labels = \n",
    "labels = [\"I-\" + label for label in labels]\n",
    "\n",
    "# create a dictionary that map an id (0... len(labels)) to its correponsing label name\n",
    "id2label = \n",
    "# create a dictionary that map the actual label name to its correponsing id\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10a763",
   "metadata": {},
   "source": [
    "The current images are not centered to the region of interest(ROI). Luckily, the authors provide the bounding box of the ROI. Thus we can apply a simple perspective transformation to the image and the corresponding bounding boxes. Checkout ***perspective_transformer.py*** to see what happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd64d1",
   "metadata": {},
   "source": [
    "Now let's first load the image and adjust the image using our ***perspective_transformer.py***. Bounding Boxes need to be scaled between 0-1000 Unfortunately, the annotators made some mistake and created bounding boxes outside of the image. We simply fix it by deleting these bounding boxes and corresponding words for the dataset entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265a2cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c390b2dff9d34a588dcd4715d7d47e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2ea425c4d843739765efc77d686c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e9b4d80562469abdaf1e2d8f86c068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(example):\n",
    "    \n",
    "    # load image into cv2\n",
    "    \n",
    "    # load the image of the attribute \"image_path\"\n",
    "    image_path = \n",
    "    \n",
    "    # read the image using cv2.imread\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "#     get region of interest (roi) and the bounding boxes (bboxes) from the example variable\n",
    "    roi = example[\"roi\"]\n",
    "    bboxes = example[\"bboxes\"]\n",
    "    # we only have a roi, when the image is missaligned\n",
    "    if roi:\n",
    "        pts = np.asarray(roi,dtype=\"float32\")\n",
    "        # apply perspective transformation\n",
    "        image, bboxes = four_point_transform(image, pts, bboxes=bboxes)\n",
    "    # fix bgr to rgb image representation\n",
    "    image =  cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    # get width and height\n",
    "    h,w = \n",
    "    # resize image to 224,224 using cv2.resize()\n",
    "    image = \n",
    "    image = np.array(image)\n",
    "\n",
    "    # LayoutLMv2 requires to adjust the bounding boxes in scale between 0-1000\n",
    "    # normalize bounding boxes using the imported method normalize_bbox   hint: use a list comprehension\n",
    "    boxes = [normalize_bbox(b,w,h) for b in bboxes]\n",
    "    labels = example[\"ner_tags\"]\n",
    "    words = example[\"tokens\"]\n",
    "    \n",
    "    # this is just some code to identify wrong annotated label in the dataset\n",
    "    for i, box in enumerate(boxes):\n",
    "        if (min(box) < 0 or max(box) > 1000) or ((box[3] - box[1]) < 0) or ((box[2] - box[0]) < 0) or words[i]==\"\":\n",
    "            del words[i]\n",
    "            del labels[i]\n",
    "            del boxes[i]\n",
    "    \n",
    "    return {\"image\":image, \"boxes\":boxes, \"labels\":labels, \"words\":words}\n",
    "\n",
    "# let's define our new features for the dataset\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(224, 224, 3)),\n",
    "    'boxes':Sequence(Sequence(Value(dtype='int64'))),\n",
    "    'labels': Sequence(ClassLabel(names=labels)),\n",
    "    'words': Sequence(Value(\"string\")),\n",
    "})\n",
    "# apply the preprocessing to all our datasets\n",
    "# use the dataset.map method and remove all prior existing columns\n",
    "train_dataset = \n",
    "test_dataset = \n",
    "validation_dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f51bfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Array3D(shape=(224, 224, 3), dtype='int64', id=None),\n",
       " 'boxes': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=ClassLabel(num_classes=30, names=['I-menu.cnt', 'I-menu.discountprice', 'I-menu.etc', 'I-menu.itemsubtotal', 'I-menu.nm', 'I-menu.num', 'I-menu.price', 'I-menu.sub_cnt', 'I-menu.sub_etc', 'I-menu.sub_nm', 'I-menu.sub_price', 'I-menu.sub_unitprice', 'I-menu.unitprice', 'I-menu.vatyn', 'I-sub_total.discount_price', 'I-sub_total.etc', 'I-sub_total.othersvc_price', 'I-sub_total.service_price', 'I-sub_total.subtotal_price', 'I-sub_total.tax_price', 'I-total.cashprice', 'I-total.changeprice', 'I-total.creditcardprice', 'I-total.emoneyprice', 'I-total.menuqty_cnt', 'I-total.menutype_cnt', 'I-total.total_etc', 'I-total.total_price', 'I-void_menu.nm', 'I-void_menu.price'], names_file=None, id=None), length=-1, id=None),\n",
       " 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1b0d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a043550",
   "metadata": {},
   "source": [
    "Now let's finaly prepare the data for the model. LayoutLMv2Processor basically bring our image into the right shape and tokenizes our words(using word piece)\n",
    "optionally, it can also apply ocr. (especially usefull for inferencing the model later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a2e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bd2f7165a14617a8a36b9901dc84a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d386b519f62f4fc792238136a3d3f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdfc05494fe4c24abefd569f7aaed0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the LayouLMv2Processor. The path is microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\"\n",
    "processor = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a2d556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1be27eb7d14c939ffd9e149a96b4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523ec0023b0a437dbb6a50364bfa9e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b372e69137e146b4b474c980ae0beb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_input(examples):\n",
    "    images = examples[\"image\"]\n",
    "    images = [np.array(image) for image in images]\n",
    "    boxes = examples[\"boxes\"]\n",
    "    labels = examples[\"labels\"]\n",
    "    words = examples[\"words\"]\n",
    "    \n",
    "    # pass all neccessary input to the processor. Check the documentation to capture all imprtant variables\n",
    "    encoded_input = processor()\n",
    "    return encoded_input\n",
    "\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': Sequence(ClassLabel(names=labels)),\n",
    "})\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(prepare_input, batched=True, features=features,remove_columns=train_dataset.column_names)\n",
    "tokenized_test_dataset = test_dataset.map(prepare_input, batched=True, features=features,remove_columns=test_dataset.column_names)\n",
    "tokenized_validation_dataset = validation_dataset.map(prepare_input, batched=True, features=features,remove_columns=validation_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aace802",
   "metadata": {},
   "source": [
    "Since we want to train our model with the TORCH version of the huggingface trainer, we define our datset format to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f99728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "tokenized_validation_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f09dcc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Array3D(shape=(3, 224, 224), dtype='int64', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'bbox': Array2D(shape=(512, 4), dtype='int64', id=None),\n",
       " 'labels': Sequence(feature=ClassLabel(num_classes=30, names=['I-menu.cnt', 'I-menu.discountprice', 'I-menu.etc', 'I-menu.itemsubtotal', 'I-menu.nm', 'I-menu.num', 'I-menu.price', 'I-menu.sub_cnt', 'I-menu.sub_etc', 'I-menu.sub_nm', 'I-menu.sub_price', 'I-menu.sub_unitprice', 'I-menu.unitprice', 'I-menu.vatyn', 'I-sub_total.discount_price', 'I-sub_total.etc', 'I-sub_total.othersvc_price', 'I-sub_total.service_price', 'I-sub_total.subtotal_price', 'I-sub_total.tax_price', 'I-total.cashprice', 'I-total.changeprice', 'I-total.creditcardprice', 'I-total.emoneyprice', 'I-total.menuqty_cnt', 'I-total.menutype_cnt', 'I-total.total_etc', 'I-total.total_price', 'I-void_menu.nm', 'I-void_menu.price'], names_file=None, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20c14d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df57a31",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7017f",
   "metadata": {},
   "source": [
    "Here we train the model using HuggingFace's Trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a56a5e",
   "metadata": {},
   "source": [
    "first, let's download the model from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7022375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adf2c4041ad423ca2e0ae422ab94c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/765M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# laod the LayoutLMv2ForTokenClassification model from HuggingFace. The path is 'microsoft/layoutlmv2-base-uncased'.\n",
    "# Don't forget to provide the number of label using the num_labels attribute\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53c57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rest of the code is already prefilled. Checkout your results from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7dc1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label=id2label\n",
    "model.config.label2id=label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb632d",
   "metadata": {},
   "source": [
    "second, we define our performance metric. here we are using seqeval. Checkout https://github.com/chakki-works/seqeval to learn more about the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da7e55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f43492e987144eb9eed8c6fec5797f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "return_entity_level_metrics = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c4c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"layoutlmv2-finetuned-cord\", # name of directory to store the checkpoints\n",
    "    save_strategy=\"epoch\", # we save our model after each epoch\n",
    "    num_train_epochs=5, # we train for a maximum of 5 epochs\n",
    "    learning_rate = 5e-5, # our learning rate\n",
    "    warmup_ratio=0.1, # total training steps used for a linear warmup from 0 to learning_rate`\n",
    "    fp16=True, # we use mixed precision (less memory consumption)\n",
    "    evaluation_strategy = \"epoch\", #  we want to see some results during training\n",
    "    per_device_train_batch_size=2, \n",
    "    load_best_model_at_end =True, # we want to get the best model after training\n",
    "    metric_for_best_model=\"f1\", # our best model is defined by it's highest f1-score\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51fa3bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_train_dataset, \n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a381c2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 10:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.262139</td>\n",
       "      <td>0.866232</td>\n",
       "      <td>0.859223</td>\n",
       "      <td>0.862713</td>\n",
       "      <td>0.866880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.949200</td>\n",
       "      <td>0.602924</td>\n",
       "      <td>0.926410</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.921513</td>\n",
       "      <td>0.935041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.404699</td>\n",
       "      <td>0.929498</td>\n",
       "      <td>0.927994</td>\n",
       "      <td>0.928745</td>\n",
       "      <td>0.948765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.283108</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.968447</td>\n",
       "      <td>0.967273</td>\n",
       "      <td>0.973468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.258026</td>\n",
       "      <td>0.974838</td>\n",
       "      <td>0.971683</td>\n",
       "      <td>0.973258</td>\n",
       "      <td>0.973925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to layoutlmv2-finetuned-cord/checkpoint-400\n",
      "Configuration saved in layoutlmv2-finetuned-cord/checkpoint-400/config.json\n",
      "Model weights saved in layoutlmv2-finetuned-cord/checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to layoutlmv2-finetuned-cord/checkpoint-800\n",
      "Configuration saved in layoutlmv2-finetuned-cord/checkpoint-800/config.json\n",
      "Model weights saved in layoutlmv2-finetuned-cord/checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to layoutlmv2-finetuned-cord/checkpoint-1200\n",
      "Configuration saved in layoutlmv2-finetuned-cord/checkpoint-1200/config.json\n",
      "Model weights saved in layoutlmv2-finetuned-cord/checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to layoutlmv2-finetuned-cord/checkpoint-1600\n",
      "Configuration saved in layoutlmv2-finetuned-cord/checkpoint-1600/config.json\n",
      "Model weights saved in layoutlmv2-finetuned-cord/checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to layoutlmv2-finetuned-cord/checkpoint-2000\n",
      "Configuration saved in layoutlmv2-finetuned-cord/checkpoint-2000/config.json\n",
      "Model weights saved in layoutlmv2-finetuned-cord/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from layoutlmv2-finetuned-cord/checkpoint-2000 (score: 0.9732576985413289).\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd796dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to layoutlmv2_cord_model\n",
      "Configuration saved in layoutlmv2_cord_model/config.json\n",
      "Model weights saved in layoutlmv2_cord_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# let's save our model. You can also directly load your model to hugginface if you want\n",
    "trainer.save_model(\"layoutlmv2_cord_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d4ad0",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11b7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e315f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  2011243GF\n",
      "  train_loss               =     0.8271\n",
      "  train_runtime            = 0:10:05.03\n",
      "  train_samples_per_second =      6.611\n",
      "  train_steps_per_second   =      3.306\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8a70d",
   "metadata": {},
   "source": [
    "Let's evaluate our model against:\n",
    "- validation dataset\n",
    "- test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03c05aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/newsroom/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "154cb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.9739\n",
      "  eval_f1                 =     0.9733\n",
      "  eval_loss               =      0.258\n",
      "  eval_precision          =     0.9748\n",
      "  eval_recall             =     0.9717\n",
      "  eval_runtime            = 0:00:02.98\n",
      "  eval_samples_per_second =     33.514\n",
      "  eval_steps_per_second   =      4.357\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2701c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040924e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49fb0281",
   "metadata": {},
   "outputs": [],
   "source": [
    " true_predictions = [\n",
    "    [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6abcc291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  test_accuracy           =     0.9664\n",
      "  test_f1                 =     0.9573\n",
      "  test_loss               =     0.2995\n",
      "  test_precision          =     0.9555\n",
      "  test_recall             =     0.9592\n",
      "  test_runtime            = 0:00:02.97\n",
      "  test_samples_per_second =     33.593\n",
      "  test_steps_per_second   =      4.367\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70b2f0",
   "metadata": {},
   "source": [
    "looks like we did our job good. These results are comparable to the original paper. See here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df70e3",
   "metadata": {},
   "source": [
    "![caption](layoutlmCordResults.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b0566",
   "metadata": {},
   "source": [
    "Thanks for reading until the end :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
